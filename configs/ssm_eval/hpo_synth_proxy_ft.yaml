#########################################
############ Main Parameters ############
#########################################
# synthesizer to use
synth: diva

#dataset to use for finetuning
dataset_name: mn20_hc_v1

# path to the checkpoint to load
path_to_ckpt: ${paths.root_dir}/checkpoints/diva_tfm_mn20_mel_e59.ckpt

# random seed for reproducibility
seed: 42

# batch size for the train and val dataloaders
batch_size: 64

# number of epochs for each trial
trainer:
  max_epochs: 50
  log_every_n_steps: 50

# number of trials to run
# the number of TPE trials is num_trials-sampler.num_startup_trials
num_trials: 100

# number of QMC startup trials to run before the TPE algorithm
num_startup_trials: 36


######################################
############ Data Configs ############
######################################
dataset:
  path_to_dataset: ${paths.root_dir}/data/datasets/eval/${synth}_${dataset_name}
  split: train
  has_mel: false

num_workers: 0

train_val_split:
  - 0.8
  - 0.2


######################################
############ Search space ############
######################################
search_space:
  ##### Adam HPs

  lr: 
    type: float
    kwargs:
      name: lr
      low: 1e-8
      high: 5e-3
      log: true
    
  # Exp decay rate for 1st moment estimate
  beta1:
    type: float
    kwargs:
      name: beta1
      low: 0.75
      high: 0.95
      step: 0.001

  # Exp decay rate for 1st moment estimate
  beta2:
    type: float
    kwargs:
      name: beta2
      low: 0.85
      high: 0.999
      step: 0.001

  # epsilon for numerical stability
  eps:
    type: float
    kwargs:
      name: eps
      low: 1e-10
      high: 1e-4
      log: true

  # weight decay
  weight_decay:
    type: float
    kwargs:
      name: weight_decay
      low: 0.0001
      high: 10.0
      log: 0.01

  # number of linear LR warmup steps
  # Diva: len(train_dataset) is 6482 if [0.8,0.2] is used (and overall train split) 
  #       hence, one epoch at batch_size=64 is 6482//64 101
  num_warmup_steps:
    type: int
    kwargs:
      name: num_warmup_steps
      low: 101
      high: 1010
      step: 101

########################################
############ Optuna configs ############
########################################
# in which direction the objective should be optimized (minimize or maximize)
direction: minimize

# metric to optimize (for optuna lightning callback)
metric_to_optimize: val/loss

sampler:
  # number of QMC startup trials to run before the TPE algorithm
  num_startup_trials: ${num_startup_trials}

  name_startup_sampler: qmc
  name: tpe

  cfg:
    _target_: optuna.samplers.TPESampler
    # The random sampling is used instead of the TPE algorithm until the given number of trials finish in the same study.
    n_startup_trials: 0
    multivariate: true
    group: true
    seed: ${seed}
    warn_independent_sampling: false

  cfg_startup:
    _target_: optuna.samplers.QMCSampler
    # can be "sobol" or "halton"
    # -> if using sobol, it is recommended that the number of trials should be set as power of two.
    qmc_type: sobol
    # Scrambling is capable of producing better Sobol sequences.
    scramble: true
    # seed for the RNG
    seed: ${seed}
    warn_independent_sampling: false

pruner:
  cfg:
    _target_: optuna.pruners.NopPruner

  name: nop


#####################################
############ Synth proxy ############
#####################################
synth_proxy:
  _target_: models.preset.tfm
  pe_type: absolute
  hidden_features: 256
  num_blocks: 6
  num_heads: 8
  mlp_factor: 4.0
  pooling_type: cls
  last_activation: ReLU
  pe_dropout_p: 0.0
  block_activation: relu
  block_dropout_p: 0.0


###################################
############ Artifacts ############
###################################
# tag for the current study and log folders
tag: ${synth}_tfm_${dataset_name}_bs${batch_size}_ft

# name of the optuna study
study_name: ${tag}_hpo


######################################
############ Hydra configs ###########
######################################
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  output_dir: ${hydra:runtime.output_dir}

hydra:
  run:
    dir: ${paths.root_dir}/logs/optuna/${tag}
  sweep:
    dir:  ${paths.root_dir}/logs/optuna/${tag}
    subdir: ${hydra.job.num}

  job: 
    chdir: True

  job_logging:
    handlers:
      file:
        filename: ${hydra.runtime.output_dir}/${hydra.job.name}.log
    formatters:
      simple:
        format: '%(message)s'


#######################################
############ wandb configs ############
#######################################
wandb:
  _target_: lightning.pytorch.loggers.wandb.WandbLogger
  # name: "" # name of the run (generated by wandb and can be changed afterward)
  save_dir: ${paths.output_dir}
  offline: False
  id: null # pass correct id to resume experiment!
  anonymous: null # enable anonymous logging
  project: ${study_name}
  log_model: False # upload lightning ckpts
  prefix: "" # a string to put at the beginning of metric keys
  group: ""
  tags: ""
  job_type: hpo