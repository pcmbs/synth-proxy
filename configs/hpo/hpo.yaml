# @package _global_

defaults:
  - _self_
  - synth: ???
  - m_preset: ???
  - sampler: qmc_tpe
  - pruner: nop

# paths to train and val datasets for HPO
path_to_train_dataset: ${paths.root_dir}/data/datasets/${synth.name}_mn04_size=1000000_seed=300_hpo_v1
path_to_val_dataset: ${paths.root_dir}/data/datasets/${synth.name}_mn04_size=65536_seed=400_hpo_val_v1

# tag for the study, which will be added to the name of the study. 
# Note that if the study already exists, it will be resumed.
tag: ???

# seed for the RNGs
seed: 42

############ Optuna configs
# number of trials to run
# if sampler is qcm-tpe, then the number of TPE trials is num_trials-sampler.num_startup_trials
num_trials: 203

# in which direction the objective should be optimized (minimize or maximize)
direction: maximize

# metric to optimize (for optuna lightning callback)
metric_to_optimize: val/mrr


############ Training configs
# batch size used for training
# mlp based: 512; gru: 256; tfm: 256
batch_size: 512

# number of workers for the train and val dataloader
num_workers: 0
num_ranks_mrr: 256 # 512

# kwargs for lightning trainer
trainer:
  max_epochs: 1
  log_every_n_steps: 50
  val_check_interval: 0.25

# Optimizer to use
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  weight_decay: 0.0

# LR scheduler
lr_scheduler:
  _target_: utils.lr_schedulers.lin_cos_scheduler_builder
  _partial_: true
  linear_start_factor: 1
  linear_end_factor: 1
  milestone: 1450

############ Artifacts
# name of the optuna study
study_name: ${synth.name}_${m_preset.name}_${tag}

############ Hydra configs
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  output_dir: ${hydra:runtime.output_dir}

hydra:
  run:
    dir: ${paths.root_dir}/logs/optuna/${synth.name}_${m_preset.name}_${tag}
  sweep:
    dir:  ${paths.root_dir}/logs/optuna/${synth.name}_${m_preset.name}_${tag}
    subdir: ${hydra.job.num}

  job: 
    chdir: True

  job_logging:
    handlers:
      file:
        filename: ${hydra.runtime.output_dir}/${hydra.job.name}.log
    formatters:
      simple:
        format: '%(message)s'

############ wandb configs
wandb:
  _target_: lightning.pytorch.loggers.wandb.WandbLogger
  # name: "" # name of the run (generated by wandb and can be changed afterward)
  save_dir: ${paths.output_dir}
  offline: False
  id: null # pass correct id to resume experiment!
  anonymous: null # enable anonymous logging
  project: ${study_name}
  log_model: False # upload lightning ckpts
  prefix: "" # a string to put at the beginning of metric keys
  group: ""
  tags: ""
  job_type: hpo