# @package _global_

defaults:
  - _self_
  - synth: ???
  - settings: defaults

######################################
############ Main Configs ############
######################################

#dataset to use for finetuning
dataset_name: mn20_hc_v1

# tag for the current study and log folders
tag: ???

# whether to use the perceptual loss or parameter loss only
has_perceptual_loss: false

# scaling factor for the cat loss (in parameter loss)
# since way bigger than the rest
cat_loss_weight: 0.01

# random seed for reproducibility
seed: 42

# batch size for the train and val dataloaders
batch_size: 64

# number of epochs for each trial
trainer:
  max_epochs: 100
  log_every_n_steps: 50

# number of trials to run
# the number of TPE trials is num_trials-sampler.num_startup_trials
num_trials: 100

# number of QMC startup trials to run before the TPE algorithm
num_startup_trials: 36

# whether to return the mel spectrograms while iterating over the dataset
# This is required for the estimator network but not for the synth proxy 
dataset:
  has_mel: true

# Load the synth proxy finetuned model (only effective if has_perceptual_loss=True)
synth:
  path_to_ckpt: ${paths.root_dir}/checkpoints/${synth.name}_tfm_mn20_hc_v1_ft.ckpt

######################################
############ Search space ############
######################################
search_space:
  ##### Adam HPs

  lr: 
    type: float
    kwargs:
      name: lr
      low: 1e-6
      high: 1e-2
      log: true
    
  # Exp decay rate for 1st moment estimate
  beta1:
    type: float
    kwargs:
      name: beta1
      low: 0.75
      high: 0.95
      step: 0.001

  # Exp decay rate for 1st moment estimate
  beta2:
    type: float
    kwargs:
      name: beta2
      low: 0.85
      high: 0.999
      step: 0.001

  # epsilon for numerical stability
  eps:
    type: float
    kwargs:
      name: eps
      low: 1e-9
      high: 1e-4
      log: true

  # weight decay
  weight_decay:
    type: float
    kwargs:
      name: weight_decay
      low: 1e-4
      high: 10.0
      log: true

  # number of linear LR warmup steps
  # Diva: len(train_dataset) is 6482 if [0.8,0.2] is used (and overall train split) 
  #       hence, one epoch at batch_size=64 is 6482//64 101
  num_warmup_steps:
    type: int
    kwargs:
      name: num_warmup_steps
      low: 101
      high: 1010
      step: 101

  # for parameter loss, categorical parameters
  label_smoothing:
    type: float
    kwargs:
      name: label_smoothing
      low: 0.0
      high: 0.5
      step: 0.01
